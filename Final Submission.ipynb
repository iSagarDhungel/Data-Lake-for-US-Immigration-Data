{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# ETL Pipeline for Immigration Data\n",
    "## Project Summary\n",
    "The goal of this project is to create an ETL pipeline using \"I94 immigration data\", \"US city demographic data\" and \"World Temperature Data\" in order to make a datawarehouse in parquet file format that is optimized for queries regarding immigration behavior.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Scope of the project and dataset description\n",
    "\n",
    "### Scope of the project\n",
    "In this project, we will aggregate \"World Demographic Data\" with \"Temperature data\" to form our first dimension table \"city_table\". Next we will aggregate city temperature data by city to form the second dimension table. The two datasets will be joined on destination city to form the fact table. The final database is optimized to query on immigration events to determine if temperature affects the selection of destination cities. Spark will be used to process the data.\n",
    "This modeling of data helps us to answer these questions:\n",
    "* Do immigrants prefer places with higher or lower foreign population?\n",
    "* Do immigration prefer higher or low temperature regions?\n",
    "* Do immigrants prefer Big or Small Cities?\n",
    "* Information about the cities, etc\n",
    "\n",
    "The datasets used in this project are described in next section.\n",
    "\n",
    "PySpark was used in this project for processing the data, but first we will use Pandas to perform exploratory analysis on the data. \n",
    "\n",
    "### Dataset Used \n",
    "The I94 immigration data comes from the US National Tourism and Trade Office. It is provided in SAS7BDAT format which is a binary database storage format. Some relevant attributes include:\n",
    "\n",
    "- i94yr: 4 digit year\n",
    "- i94mon: Month's value in number\n",
    "- i94cit: 3 digit code of origin city\n",
    "- i94port: 3 character code of destination city in US\n",
    "- arrdate: arrival date in USA\n",
    "- i94mode: Numeric Value mode of travel (air, land, sea or Not reported)\n",
    "- depdate: departure date from the USA\n",
    "- i94visa: visa codes Numeric Value(business, pleasure or student)\n",
    "- occup: occupation that would be performed in US\n",
    "\n",
    "The \"US city demographic data\" data comes from Opensoft. It is provided in csv format. Some of relevant attributes include these columns:\n",
    "- city: city name\n",
    "- state: state name\n",
    "- total population: population of city\n",
    "- race: primary race of population living in the city\n",
    "- average_household_size\n",
    "- foreign_born: no of foreigners in the city\n",
    "\n",
    "The \"World Temperature Data\" comes from Kaggle. It is also provided in CSV format. Some of the relevant attributes includes:\n",
    "- AverageTemperature: average temperature of city\n",
    "- City: city name\n",
    "- Country: country name\n",
    "- Latitude: latitude\n",
    "- Longitude = longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Reading the data & Priliminary check for NAN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographics_data = pd.read_csv(\"us-cities-demographics.csv\",sep=\";\")\n",
    "temperature_data = pd.read_csv('../../data2/GlobalLandTemperaturesByCity.csv')\n",
    "# we first read a sample from immigration data to see what type of data does the dataset contains\n",
    "immigration_data = pd.read_csv(\"immigration_data_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['City', 'State', 'Median Age', 'Male Population', 'Female Population',\n",
      "       'Total Population', 'Number of Veterans', 'Foreign-born',\n",
      "       'Average Household Size', 'State Code', 'Race', 'Count'],\n",
      "      dtype='object')\n",
      "NAN values containing columns\n",
      "Male Population           True\n",
      "Female Population         True\n",
      "Number of Veterans        True\n",
      "Foreign-born              True\n",
      "Average Household Size    True\n",
      "dtype: bool\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(demographics_data.columns)\n",
    "print(\"NAN values containing columns\")\n",
    "print(demographics_data.isna().any()[lambda x: x])\n",
    "demographics_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(demographics_data.State.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['dt', 'AverageTemperature', 'AverageTemperatureUncertainty', 'City',\n",
      "       'Country', 'Latitude', 'Longitude'],\n",
      "      dtype='object')\n",
      "NAN values containing columns\n",
      "AverageTemperature               True\n",
      "AverageTemperatureUncertainty    True\n",
      "dtype: bool\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(temperature_data.columns)\n",
    "print(\"NAN values containing columns\")\n",
    "print(temperature_data.isna().any()[lambda x: x])\n",
    "temperature_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n"
     ]
    }
   ],
   "source": [
    "print(len(temperature_data.Country.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port',\n",
      "       'arrdate', 'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa',\n",
      "       'count', 'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd',\n",
      "       'entdepu', 'matflag', 'biryear', 'dtaddto', 'gender', 'insnum',\n",
      "       'airline', 'admnum', 'fltno', 'visatype'],\n",
      "      dtype='object')\n",
      "NAN values containing columns\n",
      "i94addr     True\n",
      "depdate     True\n",
      "visapost    True\n",
      "occup       True\n",
      "entdepd     True\n",
      "entdepu     True\n",
      "matflag     True\n",
      "gender      True\n",
      "insnum      True\n",
      "airline     True\n",
      "fltno       True\n",
      "dtype: bool\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr    ...     entdepu  matflag  biryear   dtaddto  gender  \\\n",
       "0      1.0      HI    ...         NaN        M   1955.0  07202016       F   \n",
       "1      1.0      TX    ...         NaN        M   1990.0  10222016       M   \n",
       "2      1.0      FL    ...         NaN        M   1940.0  07052016       M   \n",
       "3      1.0      CA    ...         NaN        M   1991.0  10272016       M   \n",
       "4      3.0      NY    ...         NaN        M   1997.0  07042016       F   \n",
       "\n",
       "  insnum airline        admnum  fltno  visatype  \n",
       "0    NaN      JL  5.658267e+10  00782        WT  \n",
       "1    NaN     *GA  9.436200e+10  XBLNG        B2  \n",
       "2    NaN      LH  5.578047e+10  00464        WT  \n",
       "3    NaN      QR  9.478970e+10  00739        B2  \n",
       "4    NaN     NaN  4.232257e+10   LAND        WT  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(immigration_data.columns)\n",
    "print(\"NAN values containing columns\")\n",
    "print(immigration_data.isna().any()[lambda x: x])\n",
    "immigration_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since we are planning to use i94port column as joining key between the dataset and in the description it mentions the values for this column aren't consistient. We can list the values data holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['HHW', 'MCA', 'OGG', 'LOS', 'CHM', 'ATL', 'SFR', 'NYC', 'CHI',\n",
       "       'PHI', 'FTL', 'BOS', 'SAI', 'NAS', 'SEA', 'ORL', 'PSP', 'HOU',\n",
       "       'NEW', 'BAL', 'SNJ', 'DET', 'AGA', 'LVG', 'MIA', 'SDP', 'VCV',\n",
       "       'DUB', 'PEM', 'TAM', 'BLA', 'WAS', 'KOA', 'DAL', 'SHA', 'SPM',\n",
       "       'NIA', 'PHR', 'MIL', 'SLC', 'CLT', 'EPI', 'SNA', 'MON', 'DLR',\n",
       "       'SFB', 'OPF', 'X96', 'CLM', 'LIH', 'DEN', 'PHO', 'POO', 'NOL',\n",
       "       'WPB', 'PBB', 'TOR', 'MAA', 'RNO', 'FMY', 'HIG', 'OAK', 'OTM',\n",
       "       'ONT', 'SRQ', 'LLB', 'NCA', 'SUM', 'STR', 'HAM'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_data.i94port.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since all the values seems to be properly formulated except \"X96\", it is probably the case for a small sample. We can list the unique values in larger sample to properly explore the problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_data_segment = pd.read_sas('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat', 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['XXX', 'ATL', 'WAS', 'NYC', 'TOR', 'BOS', 'HOU', 'MIA', 'CHI',\n",
       "       'LOS', 'CLT', 'DEN', 'DAL', 'DET', 'NEW', 'FTL', 'LVG', 'ORL',\n",
       "       'NOL', 'PIT', 'SFR', 'SPM', 'POO', 'PHI', 'SEA', 'SLC', 'TAM',\n",
       "       'HAM', 'NAS', 'VCV', 'MAA', 'AUS', 'HHW', 'OGG', 'PHO', 'SDP',\n",
       "       'SFB', 'EDA', 'MON', 'CLG', 'DUB', 'FMY', 'YGF', 'SAJ', 'CIN',\n",
       "       'BAL', 'RDU', 'WPB', 'STT', 'OAK', 'NSV', 'SNA', 'OTT', 'X96',\n",
       "       '5KE', 'CLE', 'HAR', 'PSP', 'CHR', 'HAL', 'SAA', 'KOA', 'SHA',\n",
       "       'WIN', 'BGM', 'NCA', 'OPF', 'SAI', 'JFA', 'AGA', 'ONT', 'CLM',\n",
       "       'STL', 'W55', 'CHS', 'SNJ', 'SRQ', 'ANC', 'LNB', 'LIH', 'MIL',\n",
       "       'INP', 'KAN', 'ROC', 'SAC', 'BRO', 'LAR', 'RNO', 'SGR', 'ELP',\n",
       "       'MCA', 'MDT', 'SPE', 'FPR', 'SYR', 'ICT', 'MLB', 'ADS', 'TUC',\n",
       "       'DLR', 'CAE', 'CHA', 'HSV', 'WIL', 'HPN', 'HEF', 'BRG', 'BED',\n",
       "       'DAB', 'JAC', 'FRB', 'SWF', 'KEY', 'PTK', 'MWH', 'X44', 'MYR',\n",
       "       'APF', 'ATW', 'PVD', 'BUF', 'PIE', 'MHT', 'BDL', 'NYL', 'VNY',\n",
       "       '5T6', 'LEX', 'NOR', 'BQN', 'MEM', 'INT', 'CRQ', 'SPO', 'FOK',\n",
       "       'PEV', 'FAR', 'MAF', 'TKI', 'OMA', 'LOU', 'PHF', 'RST', 'MMU',\n",
       "       'CPX', 'SCH', 'RYY', 'PEM', 'JKM', 'LYN', 'OGD', 'NC8', 'MOB',\n",
       "       'SAV', 'HIG', 'CHL', 'WLL', 'MTH', 'AXB', 'SUM', 'ADW', 'SGJ',\n",
       "       'JMZ', 'BLA', 'SSM', 'YIP', 'EPI', 'GSP', 'BHX', 'MND', 'FCA',\n",
       "       'CRP', 'YHC', 'PHU', 'COB', 'OTM', 'STR', 'PSM', 'FWA', 'SYS',\n",
       "       'PEN', 'ABQ', 'HEL', 'DPA', 'CHM', 'EGP', 'POR', 'PIR', 'ORO',\n",
       "       'LUK', 'DER', 'DOU', 'SWE', 'NOO', 'LAN', 'VIC', 'COO', 'NRN',\n",
       "       'REN', 'HTM', 'HID', 'BEL', 'CLS', 'PRO', 'PRE', 'PCF', 'RIF',\n",
       "       'ROS', 'PAR', 'BEE', 'DAC', 'NOG', 'BTN', 'DNS', 'NAC', 'GAL',\n",
       "       'FPT', 'ABG', 'MAS', 'PTL', 'TEC', 'ROO', 'BAU', 'FRI', 'TRO',\n",
       "       'ANA', 'FRE', 'AND', 'LEW', 'NIA', 'PBB', 'THO', 'YSL', 'BEB',\n",
       "       'DLB', 'DNA', 'FTC', 'GPM', 'LAU', 'LCB', 'LLB', 'MOO', 'NEC',\n",
       "       'PHR', 'ROU', 'SKA', 'WHO', 'ANZ', 'BOA', 'CAL', 'MAD', 'MOR',\n",
       "       'ROM', 'WBE', 'AGN', 'CNA', 'SLU', 'FER', 'ALC', 'MET', 'PDN',\n",
       "       'PNH', 'VIB', 'WAL', 'BWA', 'CHT', 'DVL', 'FRT', 'HNN', 'HNS',\n",
       "       'HVR', 'SJO', 'WAR', 'COL', 'TUR', 'ABS', 'BWM', 'CNC', 'RAY',\n",
       "       'VCB', 'MGM', 'MRC', 'PGR', 'LOI', 'ADT', 'NRG', 'CRY', 'ERC',\n",
       "       'FTF', 'FTK', 'SHR', 'MAI', 'NIG', 'NRT', 'VNB', 'FAL', 'LUB',\n",
       "       'RIO', 'LWT'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_data_segment.i94port.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The dataset contains ill formulated values like \n",
    "'XXX, X96, 5T6, ML8, NC8 etc' for column i94port. We'll have to clean them before modeling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We'll need a unique id for each immigration event for which column \"cicid\" seems promising. We can check for its uniqueness as len(df) = len(df.cicid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(immigration_data_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(immigration_data_segment.cicid.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Next We will create a dictionary referencing SAS Label Description file called \"code_city_dict\" which stores all possible port codes and city names respecitively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary of valid i94port codes\n",
    "code_city_dict = {}\n",
    "city_code_dict = {}\n",
    "r_exp = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "with open('portcode_city.txt') as f:\n",
    "     for line in f:\n",
    "         match = r_exp.search(line)\n",
    "         code_city_dict[match[1]]=match[2].strip()\n",
    "         city_code_dict[match[2].strip()]=match[1]\n",
    "\n",
    "\n",
    "# Testing the dictionary format\n",
    "#code_city_dict\n",
    "#city_code_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "@udf()\n",
    "def get_portcode_from_city(city):\n",
    "    '''\n",
    "    Input: City name\n",
    "    Output: Corresponding i94port\n",
    "    '''\n",
    "    for key in code_city_dict:\n",
    "        if city.lower() in code_city_dict[key].lower():\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Cleaning Steps\n",
    "For the I94 immigration data, we want to drop all entries where the destination city code i94port is not a valid value (e.g., XXX, 99, etc) as described in I94_SAS_Labels_Description.SAS.\n",
    "For the data use in the project, we want to drop all entries with duplicate values and Null Values, and then add the i94port of the location in each entry. We clean and processes the dataframe individually as given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Cleaning Immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|299.0|2016.0|   4.0| 103.0| 103.0|    NYC|20545.0|    1.0|     NY|20550.0|  54.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1962.0|06292016|  null|  null|     OS|5.5425872433E10|00087|      WT|\n",
      "|305.0|2016.0|   4.0| 103.0| 103.0|    NYC|20545.0|    1.0|     NY|20555.0|  63.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1953.0|06292016|  null|  null|     OS|5.5425817433E10|00087|      WT|\n",
      "|496.0|2016.0|   4.0| 103.0| 103.0|    CHI|20545.0|    1.0|     IL|20548.0|  64.0|    1.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1952.0|06292016|  null|  null|     OS|5.5428623333E10|00065|      WB|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_invalid_ports(file_path):\n",
    "    '''\n",
    "    Args: \n",
    "    file_path: Path to I94 immigration data\n",
    "    Returns: \n",
    "    Spark dataframe of the data with valid i94port\n",
    "    '''\n",
    "    imm_dataframe = spark.read.format('com.github.saurfang.sas.spark').load(file_path)\n",
    "    # Filter entries where i94port is invalid\n",
    "    valid_imm_dataframe = imm_dataframe.filter(imm_dataframe.i94port.isin(list(code_city_dict.keys())))\n",
    "\n",
    "    return valid_imm_dataframe\n",
    "\n",
    "immigration_df = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat' \n",
    "immigration_df = remove_invalid_ports(immigration_df)\n",
    "immigration_df=immigration_df.dropDuplicates(['cicid'])\n",
    "immigration_df=immigration_df.filter(immigration_df.i94port != 'null')\n",
    "immigration_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+--------+-------------+--------+---------+-------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|    City|      Country|Latitude|Longitude|i94port|\n",
      "+----------+------------------+-----------------------------+--------+-------------+--------+---------+-------+\n",
      "|1856-01-01|            26.901|                        1.359|     Ife|      Nigeria|   7.23N|    4.05E|    888|\n",
      "|1852-07-01|            15.488|                        1.395|   Perth|    Australia|  31.35S|  114.97E|    PER|\n",
      "|1828-01-01|            -1.977|                        2.551| Seattle|United States|  47.42N|  121.97W|    SEA|\n",
      "|1743-11-01|             2.767|                        1.905|Hamilton|       Canada|  42.59N|   80.73W|    HAM|\n",
      "+----------+------------------+-----------------------------+--------+-------------+--------+---------+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_df=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../../data2/GlobalLandTemperaturesByCity.csv\")\n",
    "temperature_df=temperature_df.filter(temperature_df.AverageTemperature != 'NaN')\n",
    "# Remove duplicate locations\n",
    "temperature_df=temperature_df.dropDuplicates(['City', 'Country'])\n",
    "# Get corresponding port name\n",
    "temperature_df=temperature_df.withColumn(\"i94port\", get_portcode_from_city(temperature_df.City))\n",
    "# Remove entries with no iport94 code\n",
    "temperature_df=temperature_df.filter(temperature_df.i94port != 'null')\n",
    "# Show results\n",
    "temperature_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+-------+\n",
      "|       City| State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race| Count|i94port|\n",
      "+-----------+------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+-------+\n",
      "| Cincinnati|  Ohio|      32.7|         143654|           154883|          298537|             13699|       16896|                  2.08|        OH|               White|162245|    CIN|\n",
      "|Kansas City|Kansas|      33.4|          74606|            76655|          151261|              8139|       25507|                  2.71|        KS|Black or African-...| 40177|    KAN|\n",
      "|     Dayton|  Ohio|      32.8|          66631|            73966|          140597|              8465|        7381|                  2.26|        OH|               Asian|  1885|    DAB|\n",
      "|     Austin| Texas|      32.7|         475718|           456122|          931840|             37414|      181686|                   2.5|        TX|  Hispanic or Latino|327680|    AUS|\n",
      "+-----------+------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_df=spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', quote='\"', delimiter=';').load('us-cities-demographics.csv')\n",
    "# Remove duplicate locations\n",
    "demographics_df=demographics_df.dropDuplicates(['City', 'State'])\n",
    "# Get corresponding port name\n",
    "demographics_df=demographics_df.withColumn(\"i94port\", get_portcode_from_city(demographics_df.City))\n",
    "# Remove entries with no iport94 code\n",
    "demographics_df=demographics_df.filter(demographics_df.i94port != 'null')\n",
    "# Show results\n",
    "demographics_df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Our Schema contains 3 tables. 1 Fact table and 2 dimension table. The details of the table are listed below:\n",
    "\n",
    "Fact Table: `immigration_table`\n",
    "\n",
    "Dimension Table: `cities_us_table` and `immigrants_table`\n",
    "\n",
    "~~~~\n",
    "cities_us_table\n",
    " |-- city: name of city\n",
    " |-- state: name of state\n",
    " |-- port_code: code for city\n",
    " |-- total_population: total population of the city\n",
    " |-- no_of_veterans: no of veterans in the city\n",
    " |-- no_of_foreignborns: no of foreign born residents in the city\n",
    " |-- average_household_size: no of average household size in the city\n",
    " |-- race: dominant racial group in the city \n",
    " |-- average_temperature: average temperature of the city (joined from temperature data\n",
    "~~~~\n",
    "\n",
    "~~~~\n",
    "immigrants_table\n",
    " |-- cicid: unique identifier for immigration/immigrants\n",
    " |-- birthdate: birthdate of immigrant\n",
    " |-- gender: gender of immigrant\n",
    " |-- occupation: occupation immigrant adopts in US (preferably)\n",
    " |-- visa_mode: business, pleasure or student\n",
    " |-- mode_of_arrival: mode of arrival to US eg: air, land etc\n",
    " |-- arrival_date: arrival date of immigrant in US\n",
    "~~~~\n",
    "\n",
    "~~~~\n",
    "immigration_table:\n",
    " |-- year: year of immigration\n",
    " |-- month: month of immigration\n",
    " |-- source_city: source city port\n",
    " |-- destination_city: destination city\n",
    " |-- mode_of_arrival: mode of arrival to US eg: air, land etc\n",
    " |-- average_temperature: average_temperature of US city\n",
    " |-- race: dominant racial group of destination city\n",
    " |-- foreign_born_no: total no of people in the city who were foreign born\n",
    " \n",
    "~~~~\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The following steps can be performed to create etl process\n",
    "1. Select relevant columns from immigration data for `immigrants_table`\n",
    "2. Perform join operation in demographics_df and temperature_df to get average temperature\n",
    "3. Create a new dataframe `city_us_table` selecting relevant columns from joined result obtained on step 2\n",
    "4. Join demographics and temperature table with column i94port\n",
    "5. Select relevant columns for creating table `immigration_table` as fact table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# immigrants_table\n",
    "immigrants_table = immigration_df.select([\"cicid\",\"biryear\", \"gender\", \"occup\", \"i94visa\", \"i94mode\", \"arrdate\"])\n",
    "\n",
    "# drop city to resolve ambiguity\n",
    "demographics_df = demographics_df.drop(\"City\")\n",
    "#joining demographics_df and temperature_df for average temperature\n",
    "city_temperature_full= demographics_df.join(temperature_df, demographics_df.i94port == temperature_df.i94port).drop(temperature_df.i94port)\n",
    "#select columns for city_us_table\n",
    "city_us_table = city_temperature_full.select([\"City\",\"State\",\"i94port\",\"Total Population\",\"Number of Veterans\",\"Foreign-born\",\"Average Household Size\",\"Race\",\"AverageTemperature\"])\n",
    "\n",
    "#joining demographics_df and immigration_df\n",
    "immigration_demographics_full= immigration_df.join(city_temperature_full, city_temperature_full.i94port == immigration_df.i94port).drop(immigration_df.i94port)\n",
    "#selecting relevent columns for immigration table\n",
    "immigration_table = immigration_demographics_full.select([\"cicid\",\"i94yr\",\"i94mon\",\"i94cit\",\"i94port\",\"i94mode\",\"Race\",\"Foreign-born\",\"AverageTemperature\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Renaming for consistency in column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigrants_table = immigrants_table.withColumnRenamed(\"biryear\", \"birthdate\")\\\n",
    "        .withColumnRenamed(\"occup\", \"occupation\")\\\n",
    "        .withColumnRenamed(\"i94visa\", \"visa_mode\")\\\n",
    "        .withColumnRenamed(\"i94mode\", \"mode_of_arrival\")\\\n",
    "        .withColumnRenamed(\"arrdate\", \"arrival_date\")\\\n",
    "\n",
    "city_us_table = city_us_table.withColumnRenamed(\"City\", \"city\")\\\n",
    "        .withColumnRenamed(\"State\", \"state\")\\\n",
    "        .withColumnRenamed(\"i94port\", \"port_code\")\\\n",
    "        .withColumnRenamed(\"Total Population\", \"total_population\")\\\n",
    "        .withColumnRenamed(\"Number of Veterans\", \"no_of_veterans\")\\\n",
    "        .withColumnRenamed(\"Foreign-born\", \"no_of_foreignborns\")\\\n",
    "        .withColumnRenamed(\"Average Household Size\", \"average_household_size\")\\\n",
    "        .withColumnRenamed(\"Race\", \"race\")\\\n",
    "        .withColumnRenamed(\"AverageTemperature\", \"average_temperature\")\\\n",
    "        \n",
    "immigration_table = immigration_table.withColumnRenamed(\"i94yr\", \"city\")\\\n",
    "        .withColumnRenamed(\"i94mon\", \"state\")\\\n",
    "        .withColumnRenamed(\"i94port\", \"port_code\")\\\n",
    "        .withColumnRenamed(\"i94cit\", \"total_population\")\\\n",
    "        .withColumnRenamed(\"i94mode\", \"no_of_veterans\")\\\n",
    "        .withColumnRenamed(\"Foreign-born\", \"no_of_foreignborns\")\\\n",
    "        .withColumnRenamed(\"Race\", \"race\")\\\n",
    "        .withColumnRenamed(\"AverageTemperature\", \"average_temperature\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------+----------+---------+---------------+------------+\n",
      "|cicid|birthdate|gender|occupation|visa_mode|mode_of_arrival|arrival_date|\n",
      "+-----+---------+------+----------+---------+---------------+------------+\n",
      "|299.0|   1962.0|  null|      null|      2.0|            1.0|     20545.0|\n",
      "|305.0|   1953.0|  null|      null|      2.0|            1.0|     20545.0|\n",
      "|496.0|   1952.0|  null|      null|      1.0|            1.0|     20545.0|\n",
      "|558.0|   1974.0|     M|      null|      1.0|            1.0|     20545.0|\n",
      "|596.0|   1992.0|     M|      null|      2.0|            1.0|     20545.0|\n",
      "+-----+---------+------+----------+---------+---------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigrants_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+----------------+--------------+------------------+----------------------+--------------------+-------------------+\n",
      "|   city|     state|port_code|total_population|no_of_veterans|no_of_foreignborns|average_household_size|                race|average_temperature|\n",
      "+-------+----------+---------+----------------+--------------+------------------+----------------------+--------------------+-------------------+\n",
      "|Seattle|Washington|      SEA|          684443|         29364|            119840|                  2.13|  Hispanic or Latino|             -1.977|\n",
      "|Ontario|California|      ONT|          171200|          4816|             48557|                  3.52|Black or African-...|  7.399999999999999|\n",
      "|Spokane|Washington|      SPO|          213267|         18044|             13253|                  2.34|               Asian|              2.322|\n",
      "|    Ica|  Illinois|      CHI|         2720556|         72042|            573463|                  2.53|Black or African-...|              9.904|\n",
      "|Atlanta|   Georgia|      ATL|          463875|         18572|             32016|                  2.15|American Indian a...|  8.129999999999999|\n",
      "+-------+----------+---------+----------------+--------------+------------------+----------------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_us_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+----------------+---------+--------------+-----+------------------+-------------------+\n",
      "|   cicid|  city|state|total_population|port_code|no_of_veterans| race|no_of_foreignborns|average_temperature|\n",
      "+--------+------+-----+----------------+---------+--------------+-----+------------------+-------------------+\n",
      "|158829.0|2016.0|  4.0|           582.0|      SNA|           1.0|White|            208046|  7.168999999999999|\n",
      "|576038.0|2016.0|  4.0|           582.0|      SNA|           1.0|White|            208046|  7.168999999999999|\n",
      "|775234.0|2016.0|  4.0|           582.0|      SNA|           1.0|White|            208046|  7.168999999999999|\n",
      "|924449.0|2016.0|  4.0|           582.0|      SNA|           1.0|White|            208046|  7.168999999999999|\n",
      "|928755.0|2016.0|  4.0|           582.0|      SNA|           1.0|White|            208046|  7.168999999999999|\n",
      "+--------+------+-----+----------------+---------+--------------+-----+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_table.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "In order to ensure the pipeline ran as it was intended, we can run following checks and tests\n",
    " * Source/Count checks to ensure completeness\n",
    " * Listing Number of Nan Values in every columns to ensure reliability\n",
    " * Printing a sample of data obtained using show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Source/Count checks to ensure completeness - we can compare the result with original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def completeness_test(df, description):\n",
    "    '''\n",
    "    Source/Count checks to ensure completeness  \n",
    "    \n",
    "    Args:\n",
    "    df: Spark dataframe, description of Spark dataframe\n",
    "    description: output message placeholder for table name\n",
    "    \n",
    "    Returns: None\n",
    "    '''\n",
    "    result = df.count()\n",
    "    if result == 0:\n",
    "        print(\"Test Failed for {} with zero records\".format(description))\n",
    "    else:\n",
    "        print(\"Test Passed for {} with {} records\".format(description, result))\n",
    "    return None\n",
    "\n",
    "# Perform data quality check\n",
    "completeness_test(immigration_table, \"immigration table\")\n",
    "completeness_test(city_us_table, \"city table\")\n",
    "completeness_test(immigrants_table, \"immigrants table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Listing Number of Nan Values in every columns to ensure reliability. The result is expected as we have removed Nan values during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+----------------+---------+--------------+----+------------------+-------------------+\n",
      "|cicid|city|state|total_population|port_code|no_of_veterans|race|no_of_foreignborns|average_temperature|\n",
      "+-----+----+-----+----------------+---------+--------------+----+------------------+-------------------+\n",
      "|    0|   0|    0|               0|        0|             0|   0|                 0|                  0|\n",
      "+-----+----+-----+----------------+---------+--------------+----+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "immigration_table.select([count(when(isnan(c), c)).alias(c) for c in immigration_table.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---------+----------------+--------------+------------------+----------------------+----+-------------------+\n",
      "|city|state|port_code|total_population|no_of_veterans|no_of_foreignborns|average_household_size|race|average_temperature|\n",
      "+----+-----+---------+----------------+--------------+------------------+----------------------+----+-------------------+\n",
      "|   0|    0|        0|               0|             0|                 0|                     0|   0|                  0|\n",
      "+----+-----+---------+----------------+--------------+------------------+----------------------+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "city_us_table.select([count(when(isnan(c), c)).alias(c) for c in city_us_table.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------+----------+---------+---------------+------------+\n",
      "|cicid|birthdate|gender|occupation|visa_mode|mode_of_arrival|arrival_date|\n",
      "+-----+---------+------+----------+---------+---------------+------------+\n",
      "|    0|        0|     0|         0|        0|              0|           0|\n",
      "+-----+---------+------+----------+---------+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "immigrants_table.select([count(when(isnan(c), c)).alias(c) for c in immigrants_table.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "~~~~\n",
    "cities_us_table\n",
    " |-- city: name of city\n",
    " |-- state: name of state\n",
    " |-- port_code:\n",
    " |-- total_population: total population of the city\n",
    " |-- no_of_veterans: no of veterans in the city\n",
    " |-- no_of_foreignborns: no of foreign born residents in the city\n",
    " |-- average_household_size: no of average household size in the city\n",
    " |-- race: dominant racial group in the city \n",
    " |-- average_temperature: average temperature of the city (joined from temperature data)\n",
    "~~~~\n",
    "\n",
    "~~~~\n",
    "immigrants_table\n",
    " |-- cicid: unique identifier for immigration/immigrants\n",
    " |-- birthdate: birthdate of immigrant\n",
    " |-- gender: gender of immigrant\n",
    " |-- occupation: occupation immigrant adopts in US (preferably)\n",
    " |-- visa_mode: business, pleasure or student\n",
    " |-- mode_of_arrival: mode of arrival to US eg: air, land etc\n",
    " |-- arrival_date: arrival date of immigrant in US\n",
    "~~~~\n",
    "\n",
    "~~~~\n",
    "immigration_table:\n",
    " |-- year: year of immigration\n",
    " |-- month: month of immigration\n",
    " |-- source_city: source city port\n",
    " |-- destination_city: destination city\n",
    " |-- mode_of_arrival: mode of arrival to US eg: air, land etc\n",
    " |-- average_temperature: average_temperature of US city\n",
    " |-- race: dominant racial group of destination city\n",
    " |-- foreign_born_no: total no of people in the city who were foreign born\n",
    " \n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Justification of Technology and Update of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Pandas was used to explore the data since it has an intuitive and easy interface for cleaning, transforming, manipulating and analyzing data. For the production side of things (building the data pipeline), Spark(specifically pyspark) was chosen for etl processing of the data. This framework is known for its greater speed compared with the other traditional data processing frameworks because of distributed processing mechanism. Pyspark also has support for mutiple files formats like csv,parquet,etc and can be easily integrated with s3, an amazon based cloud storage service. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The update of the data depends entirely on the business requirement. The project doesn't deal with urgent analysis (for example, getting nearest ride), so it is not intuitive to know when should we should update the data. The most probable use case for the project is for some study regarding immigratants behaviour, on which it can be updated at the time of query. However, for projects like these, it might be a good idea to update the data once a month or once every 3 month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Possible Scenerios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "If the data was increased by 100x, we will require massive storage, server performance has to be optimal, and there’s an array of networking and security concerns. The most straightforward way to mitigate this concern would be to use service like Elastic Map Reduce (EMR) from AWS. Amazon EMR can also helps us to  transform and move large amounts of data into and out of other AWS data stores and databases, such as Amazon Simple Storage Service (Amazon S3) and Amazon DynamoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "If the data needs to populate a dashboard daily  then we could use a scheduling tool such as Airflow to run the ETL pipeline. It can interface with third party python APIs or Amazon Based Services to extract, transform, or load data processing steps as pipeline into the user dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "100 users accessing the dataset shouldn't be a problem after we move it to Amazon s3. The issue can arise when those users start updating and deleting the files. So first of all read only access should be done for those users. Also it is a good idea to host two buckets, one for production and the other for development to ensure further reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
